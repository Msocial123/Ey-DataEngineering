{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Load from your raw zone S3 bucket\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"s3:/buket uri/file.csv\")\n",
    "\n",
    "print(\"✅ Raw data loaded successfully.\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total rows\n",
    "total_rows = df.count()\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "\n",
    "# Count nulls per column\n",
    "null_summary = df.select([count(when(col(c).isNull() | (col(c) == '') | isnan(c), c)).alias(c) for c in df.columns])\n",
    "null_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no CustomerID\n",
    "df = df.filter(col(\"CustomerID\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Null Description with Placeholder\n",
    "df = df.fillna({'Description': 'Unknown Product'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Missing Numeric Columns (optional)\n",
    "df = df.fillna({'Quantity': 0, 'UnitPrice': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Again After Cleaning\n",
    "null_summary_after = df.select([count(when(col(c).isNull() | (col(c) == '') | isnan(c), c)).alias(c) for c in df.columns])\n",
    "null_summary_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c58405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to correct data types for analysis.\n",
    "# Data Standardization (Formatting & Consistency)\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"int\")) \\\n",
    "       .withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"double\")) \\\n",
    "       .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"int\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54934b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Text Columns (Trim, Uppercase, Remove Spaces)\n",
    "from pyspark.sql.functions import trim, upper\n",
    "\n",
    "df = df.withColumn(\"StockCode\", trim(upper(col(\"StockCode\")))) \\\n",
    "       .withColumn(\"Description\", trim(upper(col(\"Description\")))) \\\n",
    "       .withColumn(\"Country\", trim(upper(col(\"Country\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates# Remove Duplicates\n",
    "before = df.count()\n",
    "df = df.dropDuplicates()\n",
    "after = df.count()\n",
    "print(f\"Removed {before - after} duplicate rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Cleaned Dataset\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Cleaned & Standardized Data to S3 (Parquet)\n",
    "output_path = \"s3:/buket uri\"\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"✅ Cleaned and standardized data written to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
