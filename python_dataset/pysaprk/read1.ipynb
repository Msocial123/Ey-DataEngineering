{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57c688c-7ff4-4dae-baf7-ec09351b2c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DATA-AWS:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>importing data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f56e162110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Spark\\spark-3.5.7-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\"\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"importing data\").master(\"local[*]\").getOrCreate()\n",
    " \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b461b84-da2d-46a2-880d-82c7776f2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff954e5-b337-4b8a-92c2-3a2db2c80144",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = spark.read.option(\"multiline\", True).json(r'C:\\Users\\user1\\pyspark_env\\complex_pyspark_practice.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068a231e-0dbb-4563-b22e-67f6ff60657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ce016f-3d4b-4cca-b905-bce8b3abd8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clients: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- client_id: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- projects: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- company: struct (nullable = true)\n",
      " |    |-- departments: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- dept_id: string (nullable = true)\n",
      " |    |    |    |-- employees: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- designation: string (nullable = true)\n",
      " |    |    |    |    |    |-- emp_id: string (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |-- projects: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- duration_months: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- project_id: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |    |    |-- salary: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- bonus: long (nullable = true)\n",
      " |    |    |    |    |    |-- skills: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- financials: struct (nullable = true)\n",
      " |    |-- expenses: long (nullable = true)\n",
      " |    |-- fiscal_year: long (nullable = true)\n",
      " |    |-- profit: long (nullable = true)\n",
      " |    |-- revenue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonfile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd57d08c-9b15-4e62-8dd2-d03475d5bbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clients', 'company', 'financials']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonfile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4080369-bf14-4556-9240-e80862a319c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+\n",
      "|clients                                                              |company                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |financials                        |\n",
      "+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+\n",
      "|[{C101, India, Infosys, [P01, P04]}, {C102, USA, Google, [P02, P03]}]|{[{D001, [{Data Engineer, E1001, Arjun Kumar, [{6, ETL Migration, P01, Completed}, {4, Data Lake Setup, P02, Ongoing}], {75000, 10000}, [PySpark, SQL, Hadoop, Python]}, {Spark Developer, E1002, Sneha R, [{8, Streaming Pipeline, P03, Ongoing}], {82000, 12000}, [Spark Streaming, Kafka, Scala]}], Data Engineering}, {D002, [{ML Engineer, E2001, Rahul Mehta, [{10, Fraud Detection, P04, Completed}], {90000, 15000}, [Python, TensorFlow, Spark MLlib]}], Machine Learning}], {Chennai, India, Tamil Nadu}, Clahan Technologies}|{7200000, 2025, 5300000, 12500000}|\n",
      "+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonfile.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344a0f84-2bf1-4011-833c-288cb7f402d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(clients=[Row(client_id='C101', country='India', name='Infosys', projects=['P01', 'P04']), Row(client_id='C102', country='USA', name='Google', projects=['P02', 'P03'])], company=Row(departments=[Row(dept_id='D001', employees=[Row(designation='Data Engineer', emp_id='E1001', name='Arjun Kumar', projects=[Row(duration_months=6, name='ETL Migration', project_id='P01', status='Completed'), Row(duration_months=4, name='Data Lake Setup', project_id='P02', status='Ongoing')], salary=Row(base=75000, bonus=10000), skills=['PySpark', 'SQL', 'Hadoop', 'Python']), Row(designation='Spark Developer', emp_id='E1002', name='Sneha R', projects=[Row(duration_months=8, name='Streaming Pipeline', project_id='P03', status='Ongoing')], salary=Row(base=82000, bonus=12000), skills=['Spark Streaming', 'Kafka', 'Scala'])], name='Data Engineering'), Row(dept_id='D002', employees=[Row(designation='ML Engineer', emp_id='E2001', name='Rahul Mehta', projects=[Row(duration_months=10, name='Fraud Detection', project_id='P04', status='Completed')], salary=Row(base=90000, bonus=15000), skills=['Python', 'TensorFlow', 'Spark MLlib'])], name='Machine Learning')], location=Row(city='Chennai', country='India', state='Tamil Nadu'), name='Clahan Technologies'), financials=Row(expenses=7200000, fiscal_year=2025, profit=5300000, revenue=12500000))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b5aa0ca-81a8-422b-9e5e-faa5cfadf6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('clients', ArrayType(StructType([StructField('client_id', StringType(), True), StructField('country', StringType(), True), StructField('name', StringType(), True), StructField('projects', ArrayType(StringType(), True), True)]), True), True), StructField('company', StructType([StructField('departments', ArrayType(StructType([StructField('dept_id', StringType(), True), StructField('employees', ArrayType(StructType([StructField('designation', StringType(), True), StructField('emp_id', StringType(), True), StructField('name', StringType(), True), StructField('projects', ArrayType(StructType([StructField('duration_months', LongType(), True), StructField('name', StringType(), True), StructField('project_id', StringType(), True), StructField('status', StringType(), True)]), True), True), StructField('salary', StructType([StructField('base', LongType(), True), StructField('bonus', LongType(), True)]), True), StructField('skills', ArrayType(StringType(), True), True)]), True), True), StructField('name', StringType(), True)]), True), True), StructField('location', StructType([StructField('city', StringType(), True), StructField('country', StringType(), True), StructField('state', StringType(), True)]), True), StructField('name', StringType(), True)]), True), StructField('financials', StructType([StructField('expenses', LongType(), True), StructField('fiscal_year', LongType(), True), StructField('profit', LongType(), True), StructField('revenue', LongType(), True)]), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonfile.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bf628-a707-4fa5-81f7-88a2a070dc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jsonfile.write.mode(\"overwrite\").parquet(\"C:/tmp1/test_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db17eaf-ca06-42b9-98dc-8d9f59810cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqfile=spark.read.format('Parquet').load(r'C:\\Users\\user1\\pyspark_env\\result\\_temporary\\0\\task_202511062354032962255790247541795_0011_m_000000\\part-00000-f712b79c-df80-4707-9fc4-5b7ddbd8bce7-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12a341-4506-424f-b129-71a82bc51767",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqfile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608dae-53aa-45b6-990d-6499cd7ece58",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pqfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d7c13df-3fce-422a-8782-9e55d0417deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME: C:\\hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Could not find files for the given pattern(s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"HADOOP_HOME:\", os.environ.get(\"HADOOP_HOME\"))\n",
    "!where winutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff249d-f4b0-4664-8efa-f34b4ea314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"test\")], [\"id\", \"name\"])\n",
    "df.write.mode(\"overwrite\").parquet(\"C:/tmp/test_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad77f6d-9317-4338-ad86-cefb53333ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
