{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7454cd-f344-438c-8f6b-763bd69d9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Spark\\spark-3.5.7-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc98e284-0fe5-4ccc-bce9-6358a963fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a38c3a7-adfc-4566-b9bd-80e3649c1818",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2840)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2837)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2927)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpqt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[3]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m spark\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2840)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2837)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2927)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"pqt\").master(\"local[3]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf58e2e7-a6e2-4d61-a9d0-422b836757d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\user1\\pyspark_env\\student_data_clean.csv\", header=True, inferSchema=True)\n",
    "df.write.mode(\"overwrite\").parquet(r\"C:\\Users\\user1\\pyspark_env\\paruetfiles\\student_data_clean.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58d5865-9336-4114-950e-3ebecec60caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spark.read.csv(r\"C:\\Users\\user1\\pyspark_env\\student_data_with_nulls.csv\", header=True, inferSchema=True)\n",
    "d1.write.mode(\"append\").parquet(r\"C:\\Users\\user1\\pyspark_env\\paruetfiles\\std\\student_data_clean_null.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ccf26c-cb5c-4857-9d27-c51ad2bf5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_prqt=spark.read.format('parquet').load(r\"C:\\Users\\user1\\pyspark_env\\studentdata_nulls.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcf54e2-52c5-4c6a-9f59-2977b8439928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_prqt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589602a3-2ae1-4aa6-9931-e12c091dc68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_prqt.count() #to fetch the number of records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c7ca6f-acc8-49c8-8ae1-06809eb793f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         1|   Student_1|NULL|Female|           Civil|    2022-09-30|Hyderabad|    C| 70.0|\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|         9|   Student_9|NULL|Female|           Civil|    2022-06-15|    Delhi|    A| 50.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        19|  Student_19|NULL|  Male|              IT|    2022-09-09|  Chennai|    A| 60.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_prqt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273b8e34-4736-4e92-9fa8-90fd9c4bcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------+----------+---------+-----+------------------+\n",
      "|summary|        student_id|student_name|               age|gender|    course|     city|grade|             marks|\n",
      "+-------+------------------+------------+------------------+------+----------+---------+-----+------------------+\n",
      "|  count|               100|          98|                91|    92|       100|       86|   76|                94|\n",
      "|   mean|              50.5|        NULL| 20.32967032967033|  NULL|      NULL|     NULL| NULL| 72.44680851063829|\n",
      "| stddev|29.011491975882016|        NULL|1.5709511984424176|  NULL|      NULL|     NULL| NULL|14.932548717826089|\n",
      "|    min|                 1|   Student_1|              18.0|Female|     Civil|Bangalore|    A|              50.0|\n",
      "|    max|               100|  Student_99|              23.0|  Male|Mechanical|     Pune|    D|             100.0|\n",
      "+-------+------------------+------------+------------------+------+----------+---------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_prqt.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38615b04-4654-4d2c-82b0-f8252ecefd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         1|   Student_1|NULL|Female|           Civil|    2022-09-30|Hyderabad|    C| 70.0|\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|         9|   Student_9|NULL|Female|           Civil|    2022-06-15|    Delhi|    A| 50.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        19|  Student_19|NULL|  Male|              IT|    2022-09-09|  Chennai|    A| 60.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std1=std_prqt.na.drop(how='all')\n",
    "std1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada6308a-dacd-4bb7-b814-6748d9b75603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960b7217-e11d-46c2-b02e-8bf017e1451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "|        21|  Student_21|19.0|Female|             ECE|    2023-02-10|     NULL| NULL| 80.0|\n",
      "|        22|  Student_22|21.0|  Male|Computer Science|    2022-12-04|     NULL| NULL|100.0|\n",
      "|        23|  Student_23|19.0|Female|             ECE|    2022-12-16|     Pune|    A| 80.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std2=std1.na.drop(subset='age')\n",
    "std2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beced5ce-6be5-43e1-9d08-faae2b338f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515ee31c-7de2-4e57-8b08-ef9bade058cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "|        21|  Student_21|19.0|Female|             ECE|    2023-02-10|     NULL| NULL| 80.0|\n",
      "|        22|  Student_22|21.0|  Male|Computer Science|    2022-12-04|     NULL| NULL|100.0|\n",
      "|        23|  Student_23|19.0|Female|             ECE|    2022-12-16|     Pune|    A| 80.0|\n",
      "|        24|  Student_24|23.0|  Male|      Mechanical|    2022-10-29|     NULL|    B| 60.0|\n",
      "|        25|  Student_25|22.0|Female|      Mechanical|    2023-03-31|     NULL|    D| 80.0|\n",
      "|        26|  Student_26|19.0|  Male|             ECE|    2022-10-09|     Pune|    A| 80.0|\n",
      "|        27|  Student_27|19.0|Female|      Mechanical|    2022-09-20|     NULL|    A| 80.0|\n",
      "|        28|        NULL|19.0|Female|      Electrical|    2022-11-16|     Pune|    A| 50.0|\n",
      "|        29|  Student_29|19.0|  Male|             ECE|    2022-07-03|     Pune|    C| 70.0|\n",
      "|        30|  Student_30|19.0|  NULL|           Civil|    2023-02-19|Hyderabad|    C| 90.0|\n",
      "|        31|  Student_31|21.0|Female|           Civil|    2023-03-31|Hyderabad|    D| 50.0|\n",
      "|        32|  Student_32|19.0|  NULL|           Civil|    2022-06-06|Hyderabad|    D| 50.0|\n",
      "|        33|  Student_33|20.0|  NULL|             ECE|    2022-09-24|    Delhi| NULL| 60.0|\n",
      "|        34|  Student_34|20.0|  Male|              IT|    2022-07-28|Hyderabad|    B| 80.0|\n",
      "|        35|  Student_35|18.0|Female|              IT|    2022-09-12|  Chennai|    A| 90.0|\n",
      "|        36|  Student_36|19.0|Female|             ECE|    2022-06-13|     Pune|    B| 50.0|\n",
      "|        37|  Student_37|19.0|  Male|      Electrical|    2022-10-21|     Pune|    B| 50.0|\n",
      "|        38|  Student_38|18.0|Female|      Electrical|    2023-04-10|Bangalore|    A| 50.0|\n",
      "|        39|  Student_39|20.0|  Male|Computer Science|    2022-07-30|     Pune|    D| 80.0|\n",
      "|        40|  Student_40|18.0|Female|             ECE|    2023-04-04|    Delhi|    B| 80.0|\n",
      "|        41|  Student_41|22.0|  Male|      Mechanical|    2023-05-12|    Delhi|    B| NULL|\n",
      "|        42|  Student_42|21.0|  Male|              IT|    2023-01-25|Bangalore|    A| 80.0|\n",
      "|        44|  Student_44|18.0|  Male|      Mechanical|          NULL|  Chennai|    D| 60.0|\n",
      "|        45|        NULL|22.0|Female|      Mechanical|    2023-01-08|    Delhi|    B| 80.0|\n",
      "|        46|  Student_46|18.0|  Male|              IT|    2022-09-04|  Chennai| NULL| 80.0|\n",
      "|        47|  Student_47|23.0|  NULL|      Mechanical|    2023-05-06|     Pune|    B| 80.0|\n",
      "|        48|  Student_48|22.0|Female|      Mechanical|    2022-11-15|Bangalore|    C| 60.0|\n",
      "|        49|  Student_49|18.0|  Male|             ECE|    2023-01-01|  Chennai|    B| 70.0|\n",
      "|        50|  Student_50|18.0|Female|Computer Science|    2023-04-10|    Delhi|    B| 80.0|\n",
      "|        51|  Student_51|22.0|Female|Computer Science|    2023-05-23|  Chennai|    C| NULL|\n",
      "|        52|  Student_52|20.0|Female|      Electrical|    2023-02-23|  Chennai| NULL| 90.0|\n",
      "|        53|  Student_53|20.0|Female|      Mechanical|    2023-05-08|    Delhi|    C| 50.0|\n",
      "|        55|  Student_55|18.0|Female|      Mechanical|    2022-07-23|  Chennai| NULL|100.0|\n",
      "|        57|  Student_57|22.0|Female|      Electrical|    2022-10-19|Bangalore|    A| 90.0|\n",
      "|        58|  Student_58|18.0|  Male|             ECE|    2023-05-31|  Chennai|    D| 60.0|\n",
      "|        59|  Student_59|21.0|  Male|      Mechanical|    2023-05-30|     NULL|    C| 60.0|\n",
      "|        60|  Student_60|20.0|  Male|              IT|    2022-07-28|     Pune|    D| 90.0|\n",
      "|        61|  Student_61|22.0|Female|      Mechanical|    2022-12-08|     Pune|    A| 50.0|\n",
      "|        62|  Student_62|20.0|Female|           Civil|    2023-04-13|    Delhi| NULL| NULL|\n",
      "|        63|  Student_63|21.0|  NULL|              IT|    2023-04-01|     NULL|    A| 80.0|\n",
      "|        64|  Student_64|23.0|  Male|      Mechanical|    2023-02-27|     NULL|    D| 50.0|\n",
      "|        65|  Student_65|19.0|Female|              IT|    2022-06-28|     Pune| NULL| NULL|\n",
      "|        66|  Student_66|21.0|Female|Computer Science|    2023-01-06|    Delhi| NULL|100.0|\n",
      "|        67|  Student_67|23.0|Female|Computer Science|    2023-04-08|     NULL|    B| 80.0|\n",
      "|        68|  Student_68|20.0|Female|              IT|    2022-11-24|Hyderabad| NULL| 90.0|\n",
      "|        69|  Student_69|22.0|Female|Computer Science|    2022-09-07|  Chennai| NULL| 50.0|\n",
      "|        70|  Student_70|22.0|  NULL|      Electrical|    2022-06-23|    Delhi| NULL|100.0|\n",
      "|        71|  Student_71|19.0|Female|      Electrical|    2022-11-20|    Delhi|    D| 60.0|\n",
      "|        72|  Student_72|21.0|  Male|              IT|    2022-09-26|Hyderabad|    C| 70.0|\n",
      "|        74|  Student_74|23.0|Female|              IT|    2022-09-06|Bangalore|    C| 50.0|\n",
      "|        75|  Student_75|21.0|Female|      Mechanical|    2022-09-14|Hyderabad|    A|100.0|\n",
      "|        76|  Student_76|23.0|Female|      Mechanical|    2022-08-08|Bangalore| NULL| 70.0|\n",
      "|        77|  Student_77|21.0|  Male|Computer Science|    2023-05-02|Hyderabad|    D| 60.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 70 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std3=std1.na.drop(subset='age',thresh=1)\n",
    "std3.show(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b48496-628a-4d36-985e-da9528b3e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+------+----------+---------+-----+-----------------+\n",
      "|summary|        student_id|student_name|               age|gender|    course|     city|grade|            marks|\n",
      "+-------+------------------+------------+------------------+------+----------+---------+-----+-----------------+\n",
      "|  count|                91|          89|                91|    83|        91|       78|   68|               86|\n",
      "|   mean|50.637362637362635|        NULL| 20.32967032967033|  NULL|      NULL|     NULL| NULL|72.67441860465117|\n",
      "| stddev| 28.63467846414223|        NULL|1.5709511984424176|  NULL|      NULL|     NULL| NULL|15.29509099291443|\n",
      "|    min|                 2|  Student_10|              18.0|Female|     Civil|Bangalore|    A|             50.0|\n",
      "|    max|               100|  Student_98|              23.0|  Male|Mechanical|     Pune|    D|            100.0|\n",
      "+-------+------------------+------------+------------------+------+----------+---------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std3.describe().show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bf98fe-f9d1-4b50-ae23-4624aaa82039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         1|   Student_1|20.0|Female|           Civil|    2022-09-30|Hyderabad|    C| 70.0|\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|         9|   Student_9|20.0|Female|           Civil|    2022-06-15|    Delhi|    A| 50.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        19|  Student_19|20.0|  Male|              IT|    2022-09-09|  Chennai|    A| 60.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std4=std_prqt.fillna(20)\n",
    "std4.show() # depending on data you provided in the fillna parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "867fba37-10ee-4560-8045-6eae62b25d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         1|   Student_1| 0.0|Female|           Civil|    2022-09-30|Hyderabad|    C| 70.0|\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|     O|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|         9|   Student_9| 0.0|Female|           Civil|    2022-06-15|    Delhi|    A| 50.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|     O|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        19|  Student_19| 0.0|  Male|              IT|    2022-09-09|  Chennai|    A| 60.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std5=std_prqt.na.fill({'gender':'O',\"age\":0})\n",
    "std5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba793cc-ae1d-4663-a5e0-a46a21c6393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|student_id|student_name| age|gender|          course|admission_date|     city|grade|marks|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "|         1|   Student_1|NULL|Female|           Civil|    2022-09-30|Hyderabad|    C| 70.0|\n",
      "|         2|   Student_2|18.0|Female|           Civil|    2023-03-21|Bangalore|    D| 60.0|\n",
      "|         3|   Student_3|21.0|  Male|Computer Science|    2022-11-17|     Pune|    D| 70.0|\n",
      "|         4|   Student_4|22.0|  Male|      Mechanical|    2022-10-09|    Delhi|    C| 80.0|\n",
      "|         5|   Student_5|18.0|  NULL|      Electrical|    2023-02-20|Bangalore| NULL| 60.0|\n",
      "|         6|   Student_6|21.0|Female|             ECE|    2022-07-20|     Pune|    B| 70.0|\n",
      "|         7|   Student_7|21.0|  Male|              IT|    2022-11-09|Bangalore|    D| 80.0|\n",
      "|         8|   Student_8|18.0|  Male|Computer Science|    2022-12-30|    Delhi|    B| 80.0|\n",
      "|         9|   Student_9|NULL|Female|           Civil|    2022-06-15|    Delhi|    A| 50.0|\n",
      "|        10|  Student_10|20.0|  Male|             ECE|    2022-11-10|Hyderabad|    A| 80.0|\n",
      "|        11|  Student_11|21.0|  NULL|Computer Science|    2022-07-03|     NULL| NULL| 90.0|\n",
      "|        12|  Student_12|23.0|Female|           Civil|    2022-10-09|  Chennai| NULL| 80.0|\n",
      "|        13|  Student_13|20.0|Female|              IT|          NULL|    Delhi|    C| 50.0|\n",
      "|        14|  Student_14|20.0|Female|      Electrical|    2023-01-17|    Delhi|    C| 60.0|\n",
      "|        15|  Student_15|21.0|Female|      Mechanical|    2022-07-15|     NULL|    A| 50.0|\n",
      "|        16|  Student_16|19.0|  Male|      Electrical|    2023-05-14|  Chennai|    A| 60.0|\n",
      "|        17|  Student_17|18.0|  Male|             ECE|          NULL|Hyderabad|    A| 70.0|\n",
      "|        18|  Student_18|19.0|  Male|             ECE|    2022-09-04|  Chennai|    D| 70.0|\n",
      "|        19|  Student_19|NULL|  Male|              IT|    2022-09-09|  Chennai|    A| 60.0|\n",
      "|        20|  Student_20|20.0|  Male|Computer Science|    2022-10-04|Bangalore| NULL| 70.0|\n",
      "+----------+------------+----+------+----------------+--------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_prqt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4203ef1-f308-44d3-b027-1c632ff7f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#views\n",
    "#a view in a spark is like atemporory tables that exists only when the spark sessionn is active \n",
    "#you can query using standard sql Syntaax\n",
    "        #1.Local Temporary view\n",
    "        #2.global temporary view\n",
    "#cast dataframes in to --> views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06b574ec-5d27-4b91-a984-1e82e0a803b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_prqt.createTempView(\"V_Std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92167634-b703-40eb-95ea-05d22f14f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='V_Std', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94237b33-2dfb-4678-96da-31cc3d4a067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|student_id| age|\n",
      "+----------+----+\n",
      "|         2|18.0|\n",
      "|         3|21.0|\n",
      "|         4|22.0|\n",
      "|         5|18.0|\n",
      "|         6|21.0|\n",
      "|         7|21.0|\n",
      "|         8|18.0|\n",
      "|        10|20.0|\n",
      "|        11|21.0|\n",
      "|        12|23.0|\n",
      "|        13|20.0|\n",
      "|        14|20.0|\n",
      "|        15|21.0|\n",
      "|        16|19.0|\n",
      "|        17|18.0|\n",
      "|        18|19.0|\n",
      "|        20|20.0|\n",
      "|        21|19.0|\n",
      "|        22|21.0|\n",
      "|        23|19.0|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=spark.sql(\"select student_id,age from V_Std where age > 15\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88afb03e-aa00-4e2d-95ab-ce3dfb2eeba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+\n",
      "|          course|cnt|\n",
      "+----------------+---+\n",
      "|      Mechanical| 18|\n",
      "|             ECE| 17|\n",
      "|              IT| 15|\n",
      "|           Civil| 17|\n",
      "|Computer Science| 18|\n",
      "|      Electrical| 15|\n",
      "+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=spark.sql(\"select course,count(*) cnt from V_std group by course\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aea3dc44-13c7-4034-ba66-16aacc53d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course: string (nullable = true)\n",
      " |-- cnt: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.printSchema()\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31359b26-67a2-424f-a553-4be68e18a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_prqt.printSchema()\n",
    "std_prqt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e703601-8f7f-450f-a9b7-47fd062abe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DATA-AWS:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pqt</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2656a3b1cc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark1 = SparkSession.builder.appName(\"views\").master(\"local[3]\").getOrCreate()\n",
    "spark1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dda8e76-436e-4670-b284-afde1c8be6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+\n",
      "|          course|cnt|\n",
      "+----------------+---+\n",
      "|      Mechanical| 18|\n",
      "|             ECE| 17|\n",
      "|              IT| 15|\n",
      "|           Civil| 17|\n",
      "|Computer Science| 18|\n",
      "|      Electrical| 15|\n",
      "+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=spark1.sql(\"select course,count(*) cnt from V_std group by course\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e152eeaa-f67e-4148-9e46-6e46e40d38eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SparkSession stopped successfully\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\" SparkSession stopped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a60824c-670a-49b6-93eb-02d018e0e02d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `V_std` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 32;\n'Aggregate ['course], ['course, count(1) AS cnt#1610L]\n+- 'UnresolvedRelation [V_std], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect course,count(*) cnt from V_std group by course\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pyspark_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `V_std` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 32;\n'Aggregate ['course], ['course, count(1) AS cnt#1610L]\n+- 'UnresolvedRelation [V_std], [], false\n"
     ]
    }
   ],
   "source": [
    "result=spark.sql(\"select course,count(*) cnt from V_std group by course\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca9bc9fb-ae14-4e91-a002-7de22641089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.createOrReplaceGlobalTempView(\"G_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a9e64b6-9f4e-450a-8566-7c5f21a854cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f16d522-78ac-49b5-aaa6-89de36de8253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SparkSession stopped successfully\n"
     ]
    }
   ],
   "source": [
    "spark1.stop()\n",
    "print(\" SparkSession stopped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12008917-45c0-4d87-b674-0b437e4882ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
